---
title: "Class 08: Breast Cancer Analysis mini project"
author: "Rachel Galleta (A16859649)"
format: pdf
toc: true
---

## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. 

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data Import
Data was dowloaded form the class website as a CVS file.

```{r}
fna.data <- "WisconsinCancer.csv"
```

```{r}
wisc.df <-read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

The first column 'diagnosis' is the expert opinion on the sample(i.e )

```{r}
wisc.df$diagnosis
```

remove diagnosis from data for subsequenct analysis

```{r}
wisc.data<- wisc.df[,-1]
dim(wisc.data)
```

Store the diagnosis as a vector for use later when we compare our results to those form expert in the field.

```{r}
diagnosis <- factor(wisc.df$diagnosis)
```


>Q1. How many observations are in this dataset?

There are `r nrow(wisc.data)` observations/patients in the dataset

```{r}
nrow(wisc.data)
```
> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
#colnames(wisc.data)
length(grep("_mean",colnames(wisc.data)))
```

## Principal Component Analysis 

the `prcomp()` functions to do PCA has scales =False default. in general we nearly always want to set this to TRUE sour analysis is not dominated by columns /variables in our data set

that have high standard deviation and mean compared to other just because the units of measurements are on different units/scale.
we want to scale and center our data.

```{r}
wisc.pr<- prcomp(wisc.data,scale=TRUE)
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
pca <- prcomp(wisc.data, scale. = TRUE)
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
cum_var <- cumsum(var_explained)
num_pcs <- which(cum_var >= 0.70)[1]
cat("Number of principal components needed to explain at least 70% of variance:", num_pcs, "\n")

```

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
num_pcs <- which(cum_var >= 0.90)[1]
cat("Number of principal components needed to explain at least 90% of variance:", num_pcs, "\n")
```

## Interpreting PCA results 

The main PC result figure is called a "score plot" or "PC plot" or "ordination plot"...

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The only things that stand out are two different colors, but is difficult to understand the numerical values.

```{r}
plot(wisc.pr$x, col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1 ],wisc.pr$x[,3 ], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
library(ggplot2)

 ggplot(wisc.pr$x) +
   aes(PC1,PC2,col=diagnosis)+
   geom_point()
```

## Variance Explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- ( wisc.pr$sdev^2) / sum( wisc.pr$sdev^2)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


## Comunicating PCA results.

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
which(cum_var >= 0.80)[1]

```

```{r}
summary(wisc.pr)
```

## Herarchical clustering

just clustering our original data is not very informative or helpful

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist,method = "complete")
```

## Results Of hierarchical clustering

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19.5, col="red", lty=2)
```

## Selecting number of clusters

In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. 

```{r}
wisc.hclust.clusters <- cutree (wisc.hclust,k=4)  
table (wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
for (k in 2:10) {
  clusters <- cutree(wisc.hclust, k = k)
  print(table(clusters, diagnosis))
}

```

## Using different methods

>Q13. Which method gives your favorite results for the same data.dist data set? Explain your reasoning.

the method "ward.D2" is the best to create groups such that variance is minimized within clusters.  

```{r}
wisc.km <- kmeans(wisc.data, centers = 2, nstart = 20)
table(wisc.km$cluster,diagnosis)
```

```{r}
table(wisc.km$cluster,wisc.hclust.clusters)
```

## Combinding methods (PCA and clustering)

clustering the original data was not very productive.The PCA results looked promising.Here we combine the methods by clustering form our PCA results. In other words "clustering in PC Space"...

```{r}
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
diagnosis <- wisc.df$diagnosis
```


```{r}
##take the first 3PCs
wisc.pr.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust (wisc.pr.dist, method="ward.D2")
```

to get our clustering membership vector (i.e. our main clustering result we "cut" three at a desired number of k groups

```{r}
wisc.pr.dist <- dist(wisc.pr$x[,1:3])
wisc.hclust <- hclust (wisc.pr.dist, method="ward.D2")
grps <-cutree( wisc.pr.hclust, k=2)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)

```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

the four cluster model still separates the two diagnoses well.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

>Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

both clustering methods captured and separating the diagnosis  of the data accurately

```{r}
table(diagnosis)
table(wisc.hclust.clusters, diagnosis)
```
## Sensitivity/Specificity

How does this clustering grps compare to the expert diagnosis

```{r}
table(grps,diagnosis)
```
>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Based on the table and data the best specificity and sensitivity is K-means, resulting in a useful clustering before PCA.

sensitivity: TP/(TP+FN)
Specificity: TN/(TN+FN)

## Predictions
We can use our PCA model for prediction with new input patient samples
